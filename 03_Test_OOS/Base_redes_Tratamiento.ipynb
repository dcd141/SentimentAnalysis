{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analisis Red Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "#import tensorflow\n",
    "#import keras\n",
    "import wordcloud\n",
    "import nltk\n",
    "import inspect\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import RSLPStemmer #Stemmer for portugese words.\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import missingno as msno\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "import string\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,roc_auc_score,roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "#import folium\n",
    "#from folium.plugins import Fullscreen\n",
    "#import geopandas as gpd\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "stop = stopwords.words('portuguese')\n",
    "#stop.drop('nao') #Stopword already have \"Não\", just adding this because it's appear on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACA LEO LA BASE DE REDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/Users/danielacadavid/Documents/Universidad/Maestria/201920/00_Bases/brazilian-ecommerce/\")\n",
    "os.chdir(\"C:\\\\Users\\\\d.cadavid141\\\\Documents\\\\Coisas Daniela\\\\201920\\\\201920\\\\00_Bases\\\\brazilian-ecommerce\\\\\")\n",
    "#os.getcwd()\n",
    "manual=pd.read_excel('Base_manual.xlsx')\n",
    "original=pd.read_excel('Base_manual.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>palavra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>iodargírios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>aacheniano</td>\n",
       "      <td>aacheniana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>aacheniano</td>\n",
       "      <td>aachenianas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>aacheniano</td>\n",
       "      <td>aachenianos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma      palavra\n",
       "0          00  iodargírios\n",
       "1           a           aa\n",
       "2  aacheniano   aacheniana\n",
       "3  aacheniano  aachenianas\n",
       "4  aacheniano  aachenianos"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_portugues = pd.read_csv('lemmatization_pt.txt',sep=\"\\t\", header=None)\n",
    "lemma_portugues.columns = [\"lemma\", \"palavra\"]\n",
    "lemma_portugues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\d.cadavid141\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "stemmer = RSLPStemmer()\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def strip_accents(text):\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def remove_urls (text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return (text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp=list(lemma_portugues.palavra)\n",
    "llem=list(lemma_portugues.lemma)\n",
    "#llem=dict(lemma_portugues.lemma)\n",
    "#lemma=dict(lemma_portugues)\n",
    "#llem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize as word_tokenize\n",
    "stop.remove('não')\n",
    "#stop.remove('nao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying RegEx for removing urls and sites\n",
    "manual.Comment = manual.Comment.apply(remove_urls)\n",
    "\n",
    "#removing numbers\n",
    "manual.Comment = manual.Comment.str.replace('\\d+', ' ')\n",
    "\n",
    "#lower cases.\n",
    "manual.Comment = manual.Comment.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "#Removing punctuation\n",
    "manual.Comment = manual.Comment.str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "#Removing stopword\n",
    "manual.Comment = manual.Comment.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "#Tokenmize\n",
    "manual.Comment = manual.apply(lambda row: word_tokenize(row['Comment']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(manual['Comment'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario 1:\n",
      "\n",
      "['boa', 'tarde', 'fiz', 'compra', 'produto', 'namorada', 'faz', 'meses', 'produto', 'começou', 'desligar', 'sozinho', 'nao', 'quer', 'ligar', 'quero', 'fazer', 'troca', 'reembolso', 'urgente', 'nao', 'chegou', 'prazo']\n",
      "\n",
      "Comentario 50:\n",
      "\n",
      "['excelente', 'produto', 'atendeu', 'expectativas', 'recomendo']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Comentario 1:\\n')\n",
    "print(f'{links[0]}\\n')\n",
    "print(f'Comentario 50:\\n')\n",
    "print(f'{links[50]}\\n')\n",
    "#print(f'Comentario 100: \\n')\n",
    "#print(f'{links[100]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt=list(manual.Comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "getattr(tqdm, '_instances', {}).clear()  # ⬅ add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 842/842 [02:19<00:00,  6.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for myList in tqdm(lt):\n",
    "    for n, i in enumerate(myList):\n",
    "        if i in lp:\n",
    "            pos=lp.index(i)\n",
    "            item2 = llem[pos]\n",
    "            myList[n]=item2\n",
    "        else:\n",
    "            item2=i\n",
    "            myList[n]=item2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual.Comment=lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(list):\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    try:\n",
    "       # for n, i in enumerate(list):\n",
    "       #     text = unicode(n, 'utf-8')\n",
    "       #     text = unicodedata.normalize('NFD', text)\n",
    "       #     text = text.encode('ascii', 'ignore')\n",
    "       #     text = text.decode(\"utf-8\")\n",
    "       #     list[n]=text\n",
    "        text = ' '.join([str(item) for item in list ])\n",
    "        text = unicode(text, 'utf-8')\n",
    "        \n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    \n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual.Comment=lt\n",
    "\n",
    "#removing accentuation\n",
    "manual.Comment = manual.Comment.apply(strip_accents)\n",
    "\n",
    "#Tokenize again\n",
    "manual.Comment = manual.apply(lambda row: word_tokenize(row['Comment']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario 1:\n",
      "\n",
      "['bom', 'tardar', 'fazer', 'comprar', 'produto', 'namorado', 'fazer', 'mes', 'produto', 'comecar', 'desligar', 'so', 'nao', 'querer', 'ligar', 'querer', 'fazer', 'trocar', 'reembolsar', 'urgente', 'nao', 'chegar', 'prazo']\n",
      "\n",
      "Comentario 50:\n",
      "\n",
      "['excelente', 'produto', 'atender', 'expectativa', 'recomendar']\n",
      "\n",
      "Comentario 100: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "links = list(manual['Comment'].values)\n",
    "print(f'Comentario 1:\\n')\n",
    "print(f'{links[0]}\\n')\n",
    "print(f'Comentario 50:\\n')\n",
    "print(f'{links[50]}\\n')\n",
    "print(f'Comentario 100: \\n')\n",
    "#print(f'{links[100]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario 1:\n",
      "\n",
      "Boa tarde! Fiz a compra do produto para a minha namorada faz 4 meses e o produto começou a desligar sozinho e nao quer ligar. Quero fazer a troca ou reembolso urgente.  Nao chegou no prazo\n",
      "\n",
      "Comentario 50:\n",
      "\n",
      "Excelente produto atendeu minhas expectativas, eu recomendo!\n",
      "\n",
      "Comentario 100: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "org = list(original['Comment'].values)\n",
    "print(f'Comentario 1:\\n')\n",
    "print(f'{org[0]}\\n')\n",
    "print(f'Comentario 50:\\n')\n",
    "print(f'{org[50]}\\n')\n",
    "print(f'Comentario 100: \\n')\n",
    "#print(f'{org[100]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual.to_csv(\"C:\\\\Users\\\\d.cadavid141\\\\Documents\\\\Coisas Daniela\\\\201920\\\\201920\\\\00_Bases\\\\brazilian-ecommerce\\\\data_tratada_manual_grande.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
