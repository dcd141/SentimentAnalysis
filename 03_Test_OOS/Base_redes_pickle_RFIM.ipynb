{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analisis Red Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "#import tensorflow\n",
    "#import keras\n",
    "import wordcloud\n",
    "import nltk\n",
    "import inspect\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import RSLPStemmer #Stemmer for portugese words.\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import missingno as msno\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "import string\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,roc_auc_score,roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aca leo la base de datos a usar con las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\d.cadavid141\\\\Documents\\\\Coisas Daniela\\\\201920\\\\201920\\\\00_Bases\\\\brazilian-ecommerce\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables\n",
    "list_vars=pd.read_csv('feature_imp_100vars.csv')\n",
    "vars_usar=list_vars[list_vars.columns[0]][:139]\n",
    "v_random =list_vars[list_vars.columns[0]][:11]\n",
    "v_lr=list_vars[list_vars.columns[0]][:652]\n",
    "\n",
    "## Base redes\n",
    "orders=pd.read_csv('data_tratada_manual_v2.csv')\n",
    "\n",
    "## Base vacia\n",
    "rows=orders.shape[0]\n",
    "orig_cols_lr=list(vars_usar)\n",
    "orig_cols_rf=list(v_random)\n",
    "orig_col_lr=list(v_lr)\n",
    "zeros=pd.DataFrame(np.zeros((rows,len(orig_cols_lr))),columns=orig_cols_lr)\n",
    "zeros_rf=pd.DataFrame(np.zeros((rows,len(orig_cols_rf))),columns=orig_cols_rf)\n",
    "zeros_lr=pd.DataFrame(np.zeros((rows,len(orig_col_lr))),columns=orig_col_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_red=pd.DataFrame(orders)\n",
    "base_red.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar los data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zeros_dataset(orig_df, other_df):\n",
    "    rows = other_df.shape[0]\n",
    "    orig_colnames = list(orig_df.columns)\n",
    "    dtypes = dict(orig_df.dtypes)\n",
    "    d = pd.DataFrame(np.zeros((rows, len(orig_colnames))), columns=orig_colnames)\n",
    "    return d.astype(dtypes) # force the same datatypes =-)\n",
    "\n",
    "# grab both dataset columns\n",
    "def generate_dataset(orig_df, other_df):\n",
    "    colnames_2 = list(other_df.columns)\n",
    "    d = generate_zeros_dataset(orig_df, other_df)\n",
    "    d[colnames_2] = other_df.copy()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREO MI CONJUNTO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abaixar</th>\n",
       "      <th>abrir</th>\n",
       "      <th>absurdo</th>\n",
       "      <th>acabamento</th>\n",
       "      <th>acabar</th>\n",
       "      <th>aceitar</th>\n",
       "      <th>acender</th>\n",
       "      <th>acessorio</th>\n",
       "      <th>acessorios</th>\n",
       "      <th>achar</th>\n",
       "      <th>...</th>\n",
       "      <th>vivar</th>\n",
       "      <th>voltar</th>\n",
       "      <th>volumar</th>\n",
       "      <th>voz</th>\n",
       "      <th>whatsapp</th>\n",
       "      <th>xiaomi</th>\n",
       "      <th>ximo</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zap</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 845 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abaixar  abrir  absurdo  acabamento  acabar  aceitar  acender  acessorio  \\\n",
       "0      0.0    0.0      0.0         0.0     0.0      0.0      0.0        0.0   \n",
       "1      0.0    0.0      0.0         0.0     0.0      0.0      0.0        0.0   \n",
       "2      0.0    0.0      0.0         0.0     0.0      0.0      0.0        0.0   \n",
       "3      0.0    0.0      0.0         0.0     0.0      0.0      0.0        0.0   \n",
       "4      0.0    0.0      0.0         0.0     0.0      0.0      0.0        0.0   \n",
       "\n",
       "   acessorios  achar  ...  vivar  voltar  volumar  voz  whatsapp  xiaomi  \\\n",
       "0         0.0    0.0  ...    0.0     0.0      0.0  0.0       0.0     0.0   \n",
       "1         0.0    0.0  ...    0.0     0.0      0.0  0.0       0.0     0.0   \n",
       "2         0.0    0.0  ...    0.0     0.0      0.0  0.0       0.0     0.0   \n",
       "3         0.0    0.0  ...    0.0     0.0      0.0  0.0       0.0     0.0   \n",
       "4         0.0    0.0  ...    0.0     0.0      0.0  0.0       0.0     0.0   \n",
       "\n",
       "   ximo  youtube  zap  zero  \n",
       "0   0.0      0.0  0.0   0.0  \n",
       "1   0.0      0.0  0.0   0.0  \n",
       "2   0.0      0.0  0.0   0.0  \n",
       "3   0.0      0.0  0.0   0.0  \n",
       "4   0.0      0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 845 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### ACA SE CREA EL TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(orders.Comment).toarray()\n",
    "X_val_mod=pd.DataFrame(X_tfidf)\n",
    "y_val=orders.Label\n",
    "cols_manual=vectorizer.get_feature_names()\n",
    "X_val_mod.columns = cols_manual\n",
    "X_val_mod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_mod=X_val_mod[X_val_mod.columns.intersection(vars_usar)]\n",
    "\n",
    "reg_manual = generate_dataset(zeros, X_val_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_usar=139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename='finalized_model_rfim_gb.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "gb_roc_auc=roc_auc_score(y_val, loaded_model.predict(reg_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7991401054803945"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8467475192943771"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_f1_test = f1_score(y_val, loaded_model.predict(reg_manual))\n",
    "gb_f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Logistica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_usar=652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_mod=X_val_mod[X_val_mod.columns.intersection(v_lr)]\n",
    "\n",
    "lr_manual = generate_dataset(zeros_lr, X_val_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename='finalized_model_rfim_lr.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "lr_roc_auc=roc_auc_score(y_val, loaded_model.predict(lr_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7343040587021326"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8013392857142858"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_f1_test = f1_score(y_val, loaded_model.predict(lr_manual))\n",
    "lr_f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_usar=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_mod=X_val_mod[X_val_mod.columns.intersection(v_random)]\n",
    "\n",
    "rf_manual = generate_dataset(zeros_rf, X_val_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename='finalized_model_rfim_rf.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "rf_roc_auc=roc_auc_score(y_val, loaded_model.predict(rf_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.612646182068333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8542246982358404"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_f1_test = f1_score(y_val, loaded_model.predict(rf_manual))\n",
    "rf_f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
